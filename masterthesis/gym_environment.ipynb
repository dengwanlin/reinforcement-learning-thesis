{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-24T15:05:08.416732Z",
     "start_time": "2025-05-24T15:05:08.341469Z"
    }
   },
   "source": [
    "#Basic information about the game\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    \"\"\"2D网格世界环境，代理需导航至随机目标位置\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, size=5, render_mode=None):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            size (int): 网格边长（生成size x size的网格）\n",
    "            render_mode (str): 渲染模式，可选 \"human\" 或 \"rgb_array\"\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.window_size = 512  # 渲染窗口尺寸（像素）\n",
    "\n",
    "        # 定义动作空间：4个离散动作 [右, 上, 左, 下]\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # 定义观测空间：代理位置（x,y） + 目标位置（x,y）\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent\": spaces.Box(0, size-1, shape=(2,), dtype=int),\n",
    "            \"target\": spaces.Box(0, size-1, shape=(2,), dtype=int)\n",
    "        })\n",
    "\n",
    "        # 动作到方向向量的映射 [Δx, Δy]\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),   # 右\n",
    "            1: np.array([0, 1]),   # 上\n",
    "            2: np.array([-1, 0]),  # 左\n",
    "            3: np.array([0, -1]),  # 下\n",
    "        }\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None  # 渲染窗口（PyGame）\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"返回当前观察值\"\"\"\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        \"\"\"返回调试信息（可选）\"\"\"\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"重置环境，随机生成代理和目标位置\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # 随机初始化代理位置（允许任何位置）\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # 随机生成目标位置，确保不与代理初始位置重合\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        # 渲染初始化（若需要）\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"执行一个动作\"\"\"\n",
    "        direction = self._action_to_direction[action]\n",
    "\n",
    "        # 计算新位置（不越界）\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # 判断是否到达目标\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = 1 if terminated else 0  # 到达目标奖励1，否则0\n",
    "        truncated = False  # 此处不设置步数限制\n",
    "\n",
    "        # 渲染更新\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return self._get_obs(), reward, terminated, truncated, self._get_info()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"渲染环境（基于PyGame）\"\"\"\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        \"\"\"内部渲染逻辑（PyGame实现）\"\"\"\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            import pygame\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = self.window_size / self.size  # 每个网格的像素尺寸\n",
    "\n",
    "        # 绘制目标（红色）\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 绘制代理（蓝色）\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # 绘制网格线\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"关闭渲染窗口\"\"\"\n",
    "        if self.window is not None:\n",
    "            import pygame\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T15:05:17.766828Z",
     "start_time": "2025-05-24T15:05:16.773109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame  # 确保已安装pygame（pip install pygame）\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    \"\"\"2D网格世界环境，代理需导航至随机目标位置\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, size=5, render_mode=None):\n",
    "        self.size = size  # 网格尺寸（size x size）\n",
    "        self.window_size = 512  # 渲染窗口大小\n",
    "\n",
    "        # 动作空间：4个方向 [右, 上, 左, 下]\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # 观测空间：代理位置 + 目标位置\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent\": spaces.Box(0, size-1, shape=(2,), dtype=int),\n",
    "            \"target\": spaces.Box(0, size-1, shape=(2,), dtype=int)\n",
    "        })\n",
    "\n",
    "        # 动作映射 [Δx, Δy]\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # 右\n",
    "            1: np.array([0, 1]),  # 上\n",
    "            2: np.array([-1, 0]), # 左\n",
    "            3: np.array([0, -1]), # 下\n",
    "        }\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.norm(\n",
    "            self._agent_location - self._target_location, ord=1\n",
    "        )}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # 随机初始化代理位置\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # 生成不与代理重合的目标位置\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return self._get_obs(), self._get_info()\n",
    "\n",
    "    def step(self, action):\n",
    "        direction = self._action_to_direction[action]\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = 1 if terminated else 0\n",
    "        truncated = False\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return self._get_obs(), reward, terminated, truncated, self._get_info()\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = self.window_size / self.size\n",
    "\n",
    "        # 绘制目标（红色方块）\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 绘制代理（蓝色圆形）\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # 绘制网格线\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "        else:\n",
    "            return np.transpose(np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2))\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "# ------------------- 测试代码 -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建可视化环境\n",
    "    env = GridWorldEnv(size=5, render_mode=\"human\")\n",
    "\n",
    "    # 运行10步随机动作测试\n",
    "    obs, info = env.reset()\n",
    "    for _ in range(10):\n",
    "        action = env.action_space.sample()  # 随机动作\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        print(f\"位置: {obs['agent']}, 奖励: {reward}, 结束: {terminated}\")\n",
    "        if terminated:\n",
    "            obs, info = env.reset()\n",
    "\n",
    "    env.close()"
   ],
   "id": "fd7e8de30517e2be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置: [2 2], 奖励: 0, 结束: False\n",
      "位置: [2 3], 奖励: 0, 结束: False\n",
      "位置: [3 3], 奖励: 0, 结束: False\n",
      "位置: [3 2], 奖励: 0, 结束: False\n",
      "位置: [3 3], 奖励: 0, 结束: False\n",
      "位置: [3 2], 奖励: 0, 结束: False\n",
      "位置: [4 2], 奖励: 0, 结束: False\n",
      "位置: [4 3], 奖励: 0, 结束: False\n",
      "位置: [4 4], 奖励: 0, 结束: False\n",
      "位置: [4 3], 奖励: 0, 结束: False\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T01:15:33.779900Z",
     "start_time": "2025-05-22T01:15:33.759898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, size: int = 5):\n",
    "        # The size of the square grid\n",
    "        self.size = size\n",
    "\n",
    "        # Define the agent and target location; randomly chosen in `reset` and updated in `step`\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        # Dictionary maps the abstract actions to the directions on the grid\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }"
   ],
   "id": "c25e7c0680991a5f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-03T07:53:17.139825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Constructing Observations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world_env import GridWorldEnv  # 假设环境代码已保存为 grid_world_env.py\n",
    "\n",
    "# ====================== 1. 定义 Q-learning 智能体 ======================\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, gamma=0.95, epsilon=1.0, epsilon_decay=0.9995, min_epsilon=0.01):\n",
    "        self.env = env\n",
    "        self.lr = learning_rate    # 学习率\n",
    "        self.gamma = gamma        # 折扣因子\n",
    "        self.epsilon = epsilon    # 初始探索率\n",
    "        self.epsilon_decay = epsilon_decay  # 探索率衰减系数\n",
    "        self.min_epsilon = min_epsilon      # 最小探索率\n",
    "\n",
    "        # 初始化 Q-table：字典键为观测状态元组 (agent_x, agent_y, target_x, target_y)\n",
    "        self.q_table = {}\n",
    "\n",
    "    def _get_state_key(self, obs):\n",
    "        \"\"\"将观测字典转换为Q-table的键（元组）\"\"\"\n",
    "        agent = tuple(obs[\"agent\"])\n",
    "        target = tuple(obs[\"target\"])\n",
    "        return (agent + target)  # 例如：(1, 2, 3, 4)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"\"\"ε-greedy策略选择动作\"\"\"\n",
    "        state_key = self._get_state_key(obs)\n",
    "\n",
    "        # 初始化未知状态的Q值\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = np.zeros(self.env.action_space.n)\n",
    "\n",
    "        # 探索：随机选择动作\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        # 利用：选择Q值最高的动作\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_key])\n",
    "\n",
    "    def update(self, obs, action, reward, next_obs):\n",
    "        \"\"\"Q-learning更新规则\"\"\"\n",
    "        state_key = self._get_state_key(obs)\n",
    "        next_state_key = self._get_state_key(next_obs)\n",
    "\n",
    "        # 初始化下一状态的Q值（如果未知）\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = np.zeros(self.env.action_space.n)\n",
    "\n",
    "        # 计算TD目标\n",
    "        current_q = self.q_table[state_key][action]\n",
    "        max_next_q = np.max(self.q_table[next_state_key])\n",
    "        td_target = reward + self.gamma * max_next_q\n",
    "\n",
    "        # 更新Q值\n",
    "        self.q_table[state_key][action] += self.lr * (td_target - current_q)\n",
    "\n",
    "        # 衰减探索率\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# ====================== 2. 训练参数设置 ======================\n",
    "TRAIN_EPISODES = 2000   # 训练总回合数\n",
    "TEST_EPISODES = 100     # 测试回合数\n",
    "SHOW_EVERY = 500        # 每N回合渲染一次演示\n",
    "\n",
    "# 初始化环境和智能体\n",
    "env = GridWorldEnv(size=5)\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "# ====================== 3. 训练循环 ======================\n",
    "train_rewards = []\n",
    "for episode in range(TRAIN_EPISODES):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # 更新智能体\n",
    "        agent.update(obs, action, reward, next_obs)\n",
    "\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    train_rewards.append(total_reward)\n",
    "\n",
    "    # 定期展示学习效果\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(f\"Episode: {episode}, Epsilon: {agent.epsilon:.2f}, Avg Reward (last 100): {np.mean(train_rewards[-100:]):.2f}\")\n",
    "\n",
    "        # 渲染演示\n",
    "        demo_env = GridWorldEnv(size=5, render_mode=\"human\")\n",
    "        demo_obs, _ = demo_env.reset()\n",
    "        demo_done = False\n",
    "        while not demo_done:\n",
    "            action = agent.get_action(demo_obs)\n",
    "            demo_obs, _, demo_done, _, _ = demo_env.step(action)\n",
    "        demo_env.close()\n",
    "\n",
    "# ====================== 4. 评估训练结果 ======================\n",
    "# 禁用探索\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "test_rewards = []\n",
    "for _ in range(TEST_EPISODES):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    test_rewards.append(episode_reward)\n",
    "\n",
    "print(f\"\\n测试结果 ({TEST_EPISODES} 回合):\")\n",
    "print(f\"平均奖励: {np.mean(test_rewards):.2f}\")\n",
    "print(f\"达成目标比例: {sum(test_rewards)/TEST_EPISODES * 100:.1f}%\")\n",
    "\n",
    "# ====================== 5. 可视化训练曲线 ======================\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 滑动平均奖励（窗口=100）\n",
    "window_size = 100\n",
    "smoothed_rewards = [np.mean(train_rewards[i-window_size:i]) for i in range(window_size, len(train_rewards))]\n",
    "\n",
    "plt.plot(range(window_size, len(train_rewards)), smoothed_rewards, label=\"滑动平均奖励 (窗口=100)\")\n",
    "plt.xlabel(\"训练回合\")\n",
    "plt.ylabel(\"奖励\")\n",
    "plt.title(\"训练过程表现\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "bc521010859aaf65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Epsilon: 0.99, Avg Reward (last 100): 1.00\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T01:35:14.824844Z",
     "start_time": "2025-05-22T01:35:14.782841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Reset function\n",
    "# 创建环境\n",
    "env = GridWorldEnv(size=5)\n",
    "\n",
    "# 重置环境（设定种子）\n",
    "obs, info = env.reset(seed=42)\n",
    "print(\"初始观测:\", obs)  # 例如: {'agent': array([3, 2]), 'target': array([1, 4])}\n",
    "print(\"附加信息:\", info) # 例如: {'distance': 5.0}\n",
    "\n",
    "# 再次重置（相同种子，结果一致）\n",
    "obs, info = env.reset(seed=42)\n",
    "print(\"相同种子观测:\", obs)  # 与上次相同"
   ],
   "id": "ea409cf5f3e6d1e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始观测: {'agent': array([0, 3]), 'target': array([3, 2])}\n",
      "附加信息: {'distance': np.float64(4.0)}\n",
      "相同种子观测: {'agent': array([0, 3]), 'target': array([3, 2])}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T01:35:26.124177Z",
     "start_time": "2025-05-22T01:35:26.082174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Step function\n",
    "# 创建环境\n",
    "env = GridWorldEnv(size=5)\n",
    "\n",
    "# 重置环境\n",
    "obs, info = env.reset(seed=42)\n",
    "print(\"初始位置:\", obs[\"agent\"])  # 例如: [3, 2]\n",
    "\n",
    "# 执行动作（向右移动）\n",
    "action = 0\n",
    "next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "print(\"新位置:\", next_obs[\"agent\"])  # 例如: [4, 2]\n",
    "print(\"奖励:\", reward)              # 0（未到达目标）\n",
    "print(\"是否终止:\", terminated)       # False"
   ],
   "id": "5146e784c61b5b06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始位置: [0 3]\n",
      "新位置: [1 3]\n",
      "奖励: 0\n",
      "是否终止: False\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T01:42:46.326008Z",
     "start_time": "2025-05-22T01:42:44.119841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Registering and making the environment\n",
    "# main.py\n",
    "import gymnasium as gym\n",
    "\n",
    "# 创建环境实例（自动从注册表加载）\n",
    "env = gym.make(\n",
    "    \"GridWorld-v0\",\n",
    "    render_mode=\"human\",  # 开启可视化\n",
    "    size=10               # 覆盖默认参数为10x10网格\n",
    ")\n",
    "\n",
    "# 运行测试\n",
    "obs, info = env.reset()\n",
    "print(\"初始观测:\", obs)\n",
    "print(\"初始距离:\", info[\"distance\"])\n",
    "\n",
    "for step in range(20):\n",
    "    action = env.action_space.sample()  # 随机策略\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    print(f\"\\nStep {step + 1}\")\n",
    "    print(\"动作:\", [\"右\", \"上\", \"左\", \"下\"][action])\n",
    "    print(\"新位置:\", obs[\"agent\"])\n",
    "    print(\"奖励:\", reward)\n",
    "    print(\"距离:\", info[\"distance\"])\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"环境终止，重置...\")\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()"
   ],
   "id": "9f78e41cfb7a37a",
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `GridWorld` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameNotFound\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgymnasium\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgym\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# 创建环境实例（自动从注册表加载）\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGridWorld-v0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrender_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhuman\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 开启可视化\u001B[39;49;00m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m               \u001B[49m\u001B[38;5;66;43;03m# 覆盖默认参数为10x10网格\u001B[39;49;00m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# 运行测试\u001B[39;00m\n\u001B[0;32m     13\u001B[0m obs, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "File \u001B[1;32m~\\PycharmProjects\\masterthesis\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:689\u001B[0m, in \u001B[0;36mmake\u001B[1;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001B[0m\n\u001B[0;32m    686\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mid\u001B[39m, \u001B[38;5;28mstr\u001B[39m)\n\u001B[0;32m    688\u001B[0m     \u001B[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001B[39;00m\n\u001B[1;32m--> 689\u001B[0m     env_spec \u001B[38;5;241m=\u001B[39m \u001B[43m_find_spec\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(env_spec, EnvSpec)\n\u001B[0;32m    693\u001B[0m \u001B[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\masterthesis\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:533\u001B[0m, in \u001B[0;36m_find_spec\u001B[1;34m(env_id)\u001B[0m\n\u001B[0;32m    527\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    528\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the latest versioned environment `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_env_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    529\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstead of the unversioned environment `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    530\u001B[0m     )\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m env_spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 533\u001B[0m     \u001B[43m_check_version_exists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mversion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    534\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mError(\n\u001B[0;32m    535\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo registered env with id: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    536\u001B[0m     )\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m env_spec\n",
      "File \u001B[1;32m~\\PycharmProjects\\masterthesis\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:399\u001B[0m, in \u001B[0;36m_check_version_exists\u001B[1;34m(ns, name, version)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m get_env_id(ns, name, version) \u001B[38;5;129;01min\u001B[39;00m registry:\n\u001B[0;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 399\u001B[0m \u001B[43m_check_name_exists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m version \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\masterthesis\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:376\u001B[0m, in \u001B[0;36m_check_name_exists\u001B[1;34m(ns, name)\u001B[0m\n\u001B[0;32m    373\u001B[0m namespace_msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in namespace \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mns\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ns \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    374\u001B[0m suggestion_msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Did you mean: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msuggestion[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`?\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m suggestion \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 376\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mNameNotFound(\n\u001B[0;32m    377\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnvironment `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt exist\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnamespace_msg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msuggestion_msg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    378\u001B[0m )\n",
      "\u001B[1;31mNameNotFound\u001B[0m: Environment `GridWorld` doesn't exist."
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:03:06.471883Z",
     "start_time": "2025-05-23T14:03:04.095703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Using Wrappers\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "\n",
    "env = gym.make('gymnasium_env/GridWorld-v0')\n",
    "env.observation_space\n",
    "\n",
    "env.reset()\n",
    "\n",
    "wrapped_env = FlattenObservation(env)\n",
    "wrapped_env.observation_space\n",
    "\n",
    "wrapped_env.reset()\n"
   ],
   "id": "e33ba1a0d1629573",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Using Wrappers\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgymnasium\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwrappers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FlattenObservation\n\u001B[1;32m----> 4\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgymnasium_env/GridWorld-v0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m env\u001B[38;5;241m.\u001B[39mobservation_space\n\u001B[0;32m      7\u001B[0m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'gym' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "c225a29aac3f424",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
