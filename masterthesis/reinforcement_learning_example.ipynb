{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf3f20c9ded0a63",
   "metadata": {},
   "source": [
    "# Einfache Einführung in Reinforcement Learning\n",
    "\n",
    "In diesem Beispiel implementieren wir das Q-Learning-Verfahren, um einem Agenten beizubringen, sich durch eine einfache Umgebung zu bewegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dee5f5284b8a442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:30:30.528301Z",
     "start_time": "2025-05-16T14:30:30.505298Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Definition der Umgebung als einfache Grid-Welt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGridWorld\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definition der Umgebung als einfache Grid-Welt\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid = np.zeros((4, 4))  # 4x4 Gitter\n",
    "        self.terminal_states = [(0, 0), (3, 3)]  # Zielfelder\n",
    "        self.state = (3, 0)  # Startzustand\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (3, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0:  # Hoch\n",
    "            x = max(0, x-1)\n",
    "        elif action == 1:  # Rechts\n",
    "            y = min(3, y+1)\n",
    "        elif action == 2:  # Runter\n",
    "            x = min(3, x+1)\n",
    "        elif action == 3:  # Links\n",
    "            y = max(0, y-1)\n",
    "\n",
    "        self.state = (x, y)\n",
    "\n",
    "        if self.state in self.terminal_states:\n",
    "            return self.state, 1 if self.state == (3, 3) else -1, True\n",
    "        return self.state, -0.01, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa39706e37a244",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:30:28.388334Z",
     "start_time": "2025-05-16T14:30:28.284565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Total Reward: 0.98\n",
      "Episode: 200, Total Reward: 0.98\n",
      "Episode: 300, Total Reward: 0.98\n",
      "Episode: 400, Total Reward: 0.97\n",
      "Episode: 500, Total Reward: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning Algorithmus\n",
    "q_table = np.zeros((4, 4, 4))  # 4x4 Zustände, 4 Aktionen\n",
    "\n",
    "gamma = 0.9  # Diskontierungsfaktor\n",
    "alpha = 0.1  # Lernrate\n",
    "epsilon = 0.1  # Exploration vs. Ausbeutung\n",
    "\n",
    "def choose_action(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    return np.argmax(q_table[state])\n",
    "\n",
    "def learn():\n",
    "    env = GridWorld()\n",
    "    episodes = 500\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Q-Wert aktualisieren\n",
    "            q_table[state][action] += alpha * (\n",
    "                reward + gamma * np.max(q_table[next_state]) - q_table[state][action]\n",
    "            )\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daaf861d115f96a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T14:30:28.484342Z",
     "start_time": "2025-05-16T14:30:28.445339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schritt 0: Zustand: (3, 0) -> Aktion: 1 -> Nächster Zustand: (3, 1) -> Reward: -0.01\n",
      "Schritt 1: Zustand: (3, 1) -> Aktion: 1 -> Nächster Zustand: (3, 2) -> Reward: -0.01\n",
      "Schritt 2: Zustand: (3, 2) -> Aktion: 1 -> Nächster Zustand: (3, 3) -> Reward: 1\n",
      "Endgültiger Zustand:  (3, 3)\n",
      "Anzahl der Schritte:  3\n"
     ]
    }
   ],
   "source": [
    "# Agent testen\n",
    "def test_agent():\n",
    "    env = GridWorld()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done = env.step(action)\n",
    "        print(f\"Schritt {steps}: Zustand: {state} -> Aktion: {action} -> Nächster Zustand: {next_state} -> Reward: {reward}\")\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Endgültiger Zustand: \", state)\n",
    "    print(\"Anzahl der Schritte: \", steps)\n",
    "\n",
    "test_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2f8614642b3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
